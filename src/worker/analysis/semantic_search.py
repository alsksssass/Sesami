"""
Graph-RAG v2: ì‹œë§¨í‹± ì½”ë“œ ê²€ìƒ‰

AWS Bedrock ë˜ëŠ” OpenAI ì„ë² ë”© ìƒì„± ë°
OpenSearch/Qdrant ë²¡í„° ì¸ë±ì‹±

Features:
- ì½”ë“œ ì²­í‚¹ (í•¨ìˆ˜ ë‹¨ìœ„/í† í° ë‹¨ìœ„)
- ì„ë² ë”© ìƒì„± (Bedrock Titan, OpenAI)
- OpenSearch k-NN ì¸ë±ì‹±
- S3 ê¸°ë°˜ ì„ë² ë”© ìºì‹±

PDD v4.0 ì—…ë°ì´íŠ¸:
- í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ìë™ provider ì„ íƒ
- config.py í†µí•©
"""
import os
import json
import hashlib
from typing import List, Dict, Any, Optional
from pathlib import Path

import numpy as np
from config import EmbeddingConfig
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False

try:
    import boto3
    from botocore.exceptions import ClientError
    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

from opensearchpy import OpenSearch, RequestsHttpConnection


class SemanticSearch:
    """
    ì‹œë§¨í‹± ì½”ë“œ ê²€ìƒ‰ ë° ì„ë² ë”© ê´€ë¦¬

    Supported Embedding Models:
    - AWS Bedrock: amazon.titan-embed-text-v1 (1024 dim)
    - OpenAI: text-embedding-3-large (1536 dim)
    """

    def __init__(
        self,
        opensearch_endpoint: str,
        opensearch_user: str,
        opensearch_password: str,
        embedding_provider: Optional[str] = None,  # None = í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ìë™ ì„ íƒ
        embedding_model: Optional[str] = None,
        aws_region: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        s3_cache_bucket: Optional[str] = None
    ):
        """
        Args:
            opensearch_endpoint: OpenSearch ì—”ë“œí¬ì¸íŠ¸
            opensearch_user: OpenSearch ì‚¬ìš©ìëª…
            opensearch_password: OpenSearch ë¹„ë°€ë²ˆí˜¸
            embedding_provider: "bedrock" ë˜ëŠ” "openai" (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ USE_BEDROCK ê¸°ë°˜ ìë™ ì„ íƒ)
            embedding_model: ëª¨ë¸ ID (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ìë™ ì„ íƒ)
            aws_region: AWS ë¦¬ì „ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ìë™ ì„ íƒ)
            openai_api_key: OpenAI API í‚¤ (provider="openai" ì‹œ í•„ìˆ˜, Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
            s3_cache_bucket: S3 ìºì‹œ ë²„í‚· (ì„ íƒì‚¬í•­)
        """
        if not TIKTOKEN_AVAILABLE:
            raise RuntimeError("tiktoken not installed. Install with: pip install tiktoken")

        # í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ìë™ ì„¤ì • (PDD v4.0)
        if embedding_provider is None:
            config = EmbeddingConfig.get_provider()
            embedding_provider = config['provider']

            if embedding_provider == 'bedrock':
                embedding_model = embedding_model or config['model_id']
                aws_region = aws_region or config['region']
                s3_cache_bucket = s3_cache_bucket or config.get('s3_cache_bucket')
            else:  # openai
                openai_api_key = openai_api_key or config.get('api_key')
                embedding_model = embedding_model or config['model']

        # OpenSearch í´ë¼ì´ì–¸íŠ¸
        self.opensearch = OpenSearch(
            hosts=[opensearch_endpoint],
            http_auth=(opensearch_user, opensearch_password),
            use_ssl=True,
            verify_certs=False,  # ë¡œì»¬ ê°œë°œìš©, í”„ë¡œë•ì…˜ì—ì„œëŠ” True
            connection_class=RequestsHttpConnection
        )

        # ì„ë² ë”© ì„¤ì •
        self.embedding_provider = embedding_provider
        self.embedding_model = embedding_model
        self.aws_region = aws_region
        self.s3_cache_bucket = s3_cache_bucket

        # Bedrock í´ë¼ì´ì–¸íŠ¸
        if embedding_provider == "bedrock":
            if not BOTO3_AVAILABLE:
                raise RuntimeError("boto3 not installed. Install with: pip install boto3")
            self.bedrock = boto3.client('bedrock-runtime', region_name=aws_region)

        # OpenAI í´ë¼ì´ì–¸íŠ¸
        elif embedding_provider == "openai":
            if not OPENAI_AVAILABLE:
                raise RuntimeError("openai not installed. Install with: pip install openai")
            if not openai_api_key:
                raise ValueError("OpenAI API key required for provider='openai'")
            openai.api_key = openai_api_key
            self.openai_client = openai.OpenAI(api_key=openai_api_key)

        # Tokenizer (GPT-4 ê¸°ì¤€)
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

        # S3 í´ë¼ì´ì–¸íŠ¸ (ìºì‹±ìš©)
        if s3_cache_bucket and BOTO3_AVAILABLE:
            self.s3 = boto3.client('s3', region_name=aws_region)
        else:
            self.s3 = None

        print(f"âœ… SemanticSearch initialized: {embedding_provider}/{embedding_model}")

    def chunk_code(
        self,
        file_content: str,
        file_path: str,
        chunk_size: int = 200,
        overlap: int = 50
    ) -> List[Dict[str, Any]]:
        """
        ì½”ë“œ ì²­í‚¹ (í† í° ê¸°ë°˜ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)

        Args:
            file_content: íŒŒì¼ ì „ì²´ ë‚´ìš©
            file_path: íŒŒì¼ ê²½ë¡œ (ë©”íƒ€ë°ì´í„°ìš©)
            chunk_size: ì²­í¬ í¬ê¸° (í† í° ìˆ˜)
            overlap: ì˜¤ë²„ë© í¬ê¸° (í† í° ìˆ˜)

        Returns:
            chunks: [{"text": "...", "metadata": {...}}, ...]
        """
        tokens = self.tokenizer.encode(file_content)
        chunks = []

        start = 0
        chunk_idx = 0

        while start < len(tokens):
            end = min(start + chunk_size, len(tokens))
            chunk_tokens = tokens[start:end]
            chunk_text = self.tokenizer.decode(chunk_tokens)

            chunks.append({
                "text": chunk_text,
                "metadata": {
                    "file_path": file_path,
                    "chunk_index": chunk_idx,
                    "token_count": len(chunk_tokens),
                    "start_token": start,
                    "end_token": end
                }
            })

            chunk_idx += 1
            start += (chunk_size - overlap)

        print(f"  ğŸ“„ Chunked {file_path}: {len(chunks)} chunks ({len(tokens)} tokens)")
        return chunks

    def generate_embeddings(
        self,
        texts: List[str],
        use_cache: bool = True
    ) -> List[np.ndarray]:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            use_cache: S3 ìºì‹œ ì‚¬ìš© ì—¬ë¶€

        Returns:
            embeddings: [np.ndarray, ...] (ê° 1024 or 1536 ì°¨ì›)
        """
        embeddings = []

        for i, text in enumerate(texts):
            # ìºì‹œ í™•ì¸
            if use_cache and self.s3 and self.s3_cache_bucket:
                cached_embedding = self._get_cached_embedding(text)
                if cached_embedding is not None:
                    embeddings.append(cached_embedding)
                    continue

            # ì„ë² ë”© ìƒì„±
            if self.embedding_provider == "bedrock":
                embedding = self._generate_bedrock_embedding(text)
            elif self.embedding_provider == "openai":
                embedding = self._generate_openai_embedding(text)
            else:
                raise ValueError(f"Unknown embedding provider: {self.embedding_provider}")

            embeddings.append(embedding)

            # ìºì‹œ ì €ì¥
            if use_cache and self.s3 and self.s3_cache_bucket:
                self._save_embedding_to_cache(text, embedding)

            if (i + 1) % 10 == 0:
                print(f"  ğŸ”¢ Generated {i + 1}/{len(texts)} embeddings...")

        print(f"âœ… Generated {len(embeddings)} embeddings")
        return embeddings

    def _generate_bedrock_embedding(self, text: str) -> np.ndarray:
        """Bedrock Titanìœ¼ë¡œ ì„ë² ë”© ìƒì„±"""
        try:
            response = self.bedrock.invoke_model(
                modelId=self.embedding_model,
                body=json.dumps({"inputText": text})
            )

            response_body = json.loads(response['body'].read())
            embedding = np.array(response_body['embedding'], dtype=np.float32)
            return embedding

        except ClientError as e:
            print(f"âŒ Bedrock error: {e}")
            # Fallback: ì œë¡œ ë²¡í„° ë°˜í™˜
            return np.zeros(1024, dtype=np.float32)

    def _generate_openai_embedding(self, text: str) -> np.ndarray:
        """OpenAIë¡œ ì„ë² ë”© ìƒì„±"""
        try:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            embedding = np.array(response.data[0].embedding, dtype=np.float32)
            return embedding

        except Exception as e:
            print(f"âŒ OpenAI error: {e}")
            # Fallback: ì œë¡œ ë²¡í„° ë°˜í™˜
            return np.zeros(1536, dtype=np.float32)

    def _get_cache_key(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ ìºì‹œ í‚¤ ìƒì„± (SHA256 í•´ì‹œ)"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()

    def _get_cached_embedding(self, text: str) -> Optional[np.ndarray]:
        """S3ì—ì„œ ìºì‹œëœ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°"""
        cache_key = self._get_cache_key(text)
        s3_key = f"embeddings/{self.embedding_model}/{cache_key}.npy"

        try:
            obj = self.s3.get_object(Bucket=self.s3_cache_bucket, Key=s3_key)
            embedding_bytes = obj['Body'].read()
            embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
            return embedding

        except self.s3.exceptions.NoSuchKey:
            return None
        except Exception as e:
            print(f"âš ï¸  Cache read error: {e}")
            return None

    def _save_embedding_to_cache(self, text: str, embedding: np.ndarray):
        """ì„ë² ë”©ì„ S3ì— ìºì‹±"""
        cache_key = self._get_cache_key(text)
        s3_key = f"embeddings/{self.embedding_model}/{cache_key}.npy"

        try:
            self.s3.put_object(
                Bucket=self.s3_cache_bucket,
                Key=s3_key,
                Body=embedding.tobytes()
            )
        except Exception as e:
            print(f"âš ï¸  Cache write error: {e}")

    def index_to_opensearch(
        self,
        embeddings: List[np.ndarray],
        metadata: List[Dict[str, Any]],
        index_name: str = "code_embeddings"
    ) -> int:
        """
        OpenSearchì— ë²¡í„° ì¸ë±ì‹±

        Args:
            embeddings: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
            metadata: ê° ì„ë² ë”©ì˜ ë©”íƒ€ë°ì´í„°
            index_name: ì¸ë±ìŠ¤ ì´ë¦„

        Returns:
            indexed_count: ì¸ë±ì‹±ëœ ë¬¸ì„œ ìˆ˜
        """
        if len(embeddings) != len(metadata):
            raise ValueError("embeddingsì™€ metadata ê¸¸ì´ ë¶ˆì¼ì¹˜")

        # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
        if not self.opensearch.indices.exists(index=index_name):
            self._create_index(index_name, dimension=len(embeddings[0]))

        # ë²Œí¬ ì¸ë±ì‹±
        bulk_actions = []
        for i, (embedding, meta) in enumerate(zip(embeddings, metadata)):
            doc_id = meta.get('id', f"{meta['file_path']}_{i}")

            bulk_actions.append({"index": {"_index": index_name, "_id": doc_id}})
            bulk_actions.append({
                "vector": embedding.tolist(),
                "metadata": meta
            })

        # ë²Œí¬ ìš”ì²­ ì‹¤í–‰
        response = self.opensearch.bulk(body=bulk_actions)

        # ì„±ê³µ ì¹´ìš´íŠ¸
        indexed_count = sum(
            1 for item in response['items']
            if item['index']['status'] in [200, 201]
        )

        print(f"âœ… Indexed {indexed_count}/{len(embeddings)} documents to '{index_name}'")
        return indexed_count

    def _create_index(self, index_name: str, dimension: int):
        """OpenSearch k-NN ì¸ë±ìŠ¤ ìƒì„±"""
        index_body = {
            "settings": {
                "index": {
                    "knn": True,
                    "knn.algo_param.ef_search": 100
                }
            },
            "mappings": {
                "properties": {
                    "vector": {
                        "type": "knn_vector",
                        "dimension": dimension,
                        "method": {
                            "name": "hnsw",
                            "space_type": "cosinesimil",
                            "engine": "nmslib",
                            "parameters": {
                                "ef_construction": 128,
                                "m": 24
                            }
                        }
                    },
                    "metadata": {
                        "type": "object"
                    }
                }
            }
        }

        self.opensearch.indices.create(index=index_name, body=index_body)
        print(f"âœ… Created OpenSearch index: {index_name} ({dimension} dim)")

    def query(
        self,
        natural_language_query: str,
        k: int = 5,
        filter_dict: Optional[Dict[str, Any]] = None,
        index_name: str = "code_embeddings"
    ) -> List[Dict[str, Any]]:
        """
        ìì—°ì–´ ì¿¼ë¦¬ë¡œ ìœ ì‚¬ ì½”ë“œ ê²€ìƒ‰

        Args:
            natural_language_query: ìì—°ì–´ ì§ˆë¬¸
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            filter_dict: ë©”íƒ€ë°ì´í„° í•„í„° (ì˜ˆ: {"analysis_id": "..."})
            index_name: ê²€ìƒ‰í•  ì¸ë±ìŠ¤

        Returns:
            results: [{"score": ..., "metadata": {...}, "text": "..."}, ...]
        """
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.generate_embeddings([natural_language_query])[0]

        # k-NN ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_body = {
            "size": k,
            "query": {
                "knn": {
                    "vector": {
                        "vector": query_embedding.tolist(),
                        "k": k
                    }
                }
            }
        }

        # í•„í„° ì¶”ê°€
        if filter_dict:
            search_body["query"] = {
                "bool": {
                    "must": [
                        {"knn": {"vector": {"vector": query_embedding.tolist(), "k": k}}}
                    ],
                    "filter": [
                        {"term": {f"metadata.{key}": value}}
                        for key, value in filter_dict.items()
                    ]
                }
            }

        # ê²€ìƒ‰ ì‹¤í–‰
        response = self.opensearch.search(index=index_name, body=search_body)

        # ê²°ê³¼ ë³€í™˜
        results = []
        for hit in response['hits']['hits']:
            results.append({
                "score": hit['_score'],
                "metadata": hit['_source']['metadata'],
                "text": hit['_source']['metadata'].get('chunk_text', '')
            })

        print(f"âœ… Query returned {len(results)} results")
        return results
