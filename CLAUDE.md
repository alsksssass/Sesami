# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**Sesami**ëŠ” GitHub ê¸°ì—¬ë„ ë¶„ì„ í”Œë«í¼ìœ¼ë¡œ, **Graph-RAG ê¸°ë°˜ ì½”ë“œ ì§€ì‹ ê·¸ë˜í”„**ë¥¼ í™œìš©í•˜ì—¬ ê°œë°œìì˜ ì •ëŸ‰ì Â·ì •ì„±ì  ê¸°ì—¬ë„ë¥¼ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤.

**í•µì‹¬ ê¸°ëŠ¥ (v2 ì—…ê·¸ë ˆì´ë“œ)**:
- GitHub OAuth ì¸ì¦ìœ¼ë¡œ Private Repository ì ‘ê·¼
- `git blame` ê¸°ë°˜ ìë™ ê¸°ì—¬ë„ ë¶„ì„
- **Graph-RAG**: ì½”ë“œ ì§€ì‹ ê·¸ë˜í”„(Neo4j) + ì‹œë§¨í‹± ê²€ìƒ‰ìœ¼ë¡œ êµì°¨ íŒŒì¼ ì•„í‚¤í…ì²˜ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
- **ì‹œë§¨í‹± ì½”ë“œ ê²€ìƒ‰**: OpenSearch/Qdrant ê¸°ë°˜ ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ ìì—°ì–´ ì¿¼ë¦¬ ì§€ì›
- ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„ ë° ì‹œê³„ì—´ íŠ¸ë Œë“œ ì¶”ì 

**v2 ì£¼ìš” ë³€ê²½ì‚¬í•­**:
- **ì§€ì‹ ê·¸ë˜í”„**: Tree-sitter íŒŒì„œë¡œ AST ì¶”ì¶œ â†’ Neo4jì— ì½”ë“œ êµ¬ì¡°(íŒŒì¼, í•¨ìˆ˜, í´ë˜ìŠ¤, í˜¸ì¶œ ê´€ê³„) ì €ì¥
- **ì‹œë§¨í‹± ì¸ë±ìŠ¤**: AWS Bedrock/OpenAI ì„ë² ë”© â†’ OpenSearch Serverlessì— ë²¡í„° ì €ì¥
- **ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: Step Functions(L1 ì»¨íŠ¸ë¡¤ëŸ¬ + L2 íŒ¬ì•„ì›ƒ Map) + AWS Batch(L3 íˆ´ ëŸ¬ë„ˆ)
- **ê´€ì¸¡ì„±**: CloudWatch + X-Ray/OpenTelemetry ê¸°ë°˜ ë¶„ì‚° ì¶”ì 

**3-Tier ë¹„ë™ê¸° ì•„í‚¤í…ì²˜**:
- **Frontend**: React 19 + Vite + Tailwind CSS + TypeScript
- **Backend**: FastAPI + PostgreSQL + SQLAlchemy 2.0
- **Worker**: Celery (ë¡œì»¬) â†’ AWS Batch (í”„ë¡œë•ì…˜)
- **Queue**: Redis (ë¡œì»¬) â†’ Amazon SQS (í”„ë¡œë•ì…˜)
- **Graph Store**: Neo4j AuraDB (ë˜ëŠ” Neptune Serverless v2)
- **Vector Store**: Amazon OpenSearch Serverless (ë˜ëŠ” Qdrant)
- **Shared Storage**: EFS (í´ë¡  ì €ì¥ì†Œ, JSONL), S3 (ì„ë² ë”© ìºì‹œ)

**í•µì‹¬ ì„¤ê³„ íŒ¨í„´**: `TaskService` ì¶”ìƒí™”ë¡œ ë¡œì»¬â†”AWS í™˜ê²½ ì „í™˜ ì‹œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ìˆ˜ì • ì—†ì´ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ëŠ¥

**ì œí’ˆ KPI (v2)**:
- ì¸ì‚¬ì´íŠ¸ ê¹Šì´: 2GB ë¯¸ë§Œ ì €ì¥ì†Œ 90%ë¥¼ 90ì´ˆ ì´ë‚´ ë¶„ì„
- ì—ì´ì „íŠ¸ ì‹ ë¢°ë„: L2/L3 ì‘ì—… ì‹¤íŒ¨ìœ¨ 5% ì´í•˜
- ë¹„ìš© íš¨ìœ¨ì„±: ë¶„ì„ 1ê±´ë‹¹ 2ë‹¬ëŸ¬ ì´í•˜

---

## Development Commands

### ë¡œì»¬ í™˜ê²½ ì‹œì‘
```bash
# Docker Composeë¡œ ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰
docker-compose up --build
docker-compose up -d --build  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰

# Graph-RAG ê°œë°œ í™˜ê²½ (v2)
make graph-dev  # Neo4j + OpenSearch ë¡œì»¬ ì»¨í…Œì´ë„ˆ í¬í•¨

# ë¡œê·¸ í™•ì¸
docker-compose logs -f
docker-compose logs -f backend
docker-compose logs -f worker
```

### ì„œë¹„ìŠ¤ ì ‘ì†
- **Frontend**: http://localhost:3000
- **Backend API Docs**: http://localhost:8000/docs
- **PostgreSQL**: `psql postgresql://sesami_user:sesami_password_2025@localhost:5432/sesami_db`
- **Redis**: `redis-cli -h localhost -p 6379`
- **Neo4j Browser** (ë¡œì»¬): http://localhost:7474
- **OpenSearch Dashboards** (ë¡œì»¬): http://localhost:5601

### ì»¨í…Œì´ë„ˆ ì…¸ ì ‘ì†
```bash
docker-compose exec backend bash
docker-compose exec frontend sh
docker-compose exec worker bash
docker-compose exec db psql -U sesami_user sesami_db
docker-compose exec neo4j cypher-shell -u neo4j -p password
```

### ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜
```bash
# Backend ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì‹¤í–‰
docker-compose exec backend bash

# ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„± (graph_snapshot í…Œì´ë¸” í¬í•¨)
alembic revision --autogenerate -m "migration description"

# ë§ˆì´ê·¸ë ˆì´ì…˜ ì ìš©
alembic upgrade head

# ë¡¤ë°±
alembic downgrade -1

# í˜„ì¬ ìƒíƒœ í™•ì¸
alembic current
```

### Worker ëª¨ë‹ˆí„°ë§
```bash
# í™œì„± ì‘ì—… í™•ì¸
docker-compose exec worker celery -A celery_app inspect active

# Worker ìƒíƒœ
docker-compose exec worker celery -A celery_app status

# Redis í ê¹Šì´ í™•ì¸
docker-compose exec queue redis-cli LLEN celery
```

### ê·¸ë˜í”„ ì‘ì—… (v2)
```bash
# Neo4j ë°ì´í„° ì„í¬íŠ¸ (ì´ˆê¸° ì ì¬)
docker-compose exec neo4j neo4j-admin database import \
  --nodes=/import/graph_nodes.jsonl \
  --relationships=/import/graph_edges.jsonl

# ê·¸ë˜í”„ ìŠ¤ëƒ…ìƒ· í™•ì¸
docker-compose exec backend python -c "
from common.database import SessionLocal
db = SessionLocal()
snapshots = db.execute('SELECT * FROM graph_snapshot').fetchall()
print(snapshots)
"
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# Backend í…ŒìŠ¤íŠ¸
docker-compose exec backend pytest
docker-compose exec backend pytest tests/test_auth.py -v
docker-compose exec backend pytest tests/test_graph_loader.py -v  # v2 ì¶”ê°€

# Frontend í…ŒìŠ¤íŠ¸
docker-compose exec frontend npm test
docker-compose exec frontend npm run lint
docker-compose exec frontend npm run build

# í†µí•© ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸
make ci
```

### í™˜ê²½ ì •ë¦¬
```bash
# ì»¨í…Œì´ë„ˆ ì¤‘ì§€
docker-compose down

# ì»¨í…Œì´ë„ˆ + ë³¼ë¥¨ ì‚­ì œ (âš ï¸ DB + Graph ë°ì´í„° ì‚­ì œ)
docker-compose down -v

# ìºì‹œ ë¬´íš¨í™” ì¬ë¹Œë“œ
docker-compose build --no-cache
docker-compose up --build --force-recreate
```

---

## Architecture Deep Dive

### 1. TaskService ì¶”ìƒí™” íŒ¨í„´ (í•µì‹¬)

**ëª©ì **: ë¡œì»¬ ê°œë°œ(Celery)ê³¼ AWS í”„ë¡œë•ì…˜(SQS+Batch) í™˜ê²½ì—ì„œ ë™ì¼í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‚¬ìš©

**ìœ„ì¹˜**: `src/backend/common/task_service/`

**êµ¬ì¡°**:
```
task_service/
â”œâ”€â”€ base.py              # ITaskService ì¸í„°í˜ì´ìŠ¤ ì •ì˜
â”œâ”€â”€ local_service.py     # LocalTaskService (Celery + Redis)
â””â”€â”€ aws_batch_service.py # AwsBatchTaskService (SQS + AWS Batch) - ğŸ”¨ Phase 1 êµ¬í˜„ ì˜ˆì •
```

**í•µì‹¬ ì¸í„°í˜ì´ìŠ¤**:
```python
class ITaskService(ABC):
    @abstractmethod
    async def enqueue_analysis(
        self,
        analysis_id: UUID,
        repo_url: str,
        target_user: str
    ) -> str:
        """ë¶„ì„ ì‘ì—…ì„ íì— ì¶”ê°€"""
        pass
```

**í™˜ê²½ ì „í™˜ ë¡œì§**:
- í™˜ê²½ë³€ìˆ˜ `TASK_SERVICE_IMPL` ê°’ì— ë”°ë¼ êµ¬í˜„ì²´ ì„ íƒ
- `"LOCAL"` â†’ `LocalTaskService` (Celery)
- `"AWS_BATCH"` â†’ `AwsBatchTaskService` (SQS + Step Functions)

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# Backend APIì—ì„œ
from common.dependencies import get_task_service

@router.post("/analysis/start")
async def start_analysis(
    task_service: ITaskService = Depends(get_task_service)
):
    job_id = await task_service.enqueue_analysis(...)
    return {"job_id": job_id}
```

### 2. Graph-RAG ì•„í‚¤í…ì²˜ (v2 í•µì‹¬)

**ì§€ì‹ ê·¸ë˜í”„ ìŠ¤í‚¤ë§ˆ** (Neo4j):

| ë…¸ë“œ íƒ€ì… | í•„ìˆ˜ ì†ì„± | ê´€ê³„ |
|----------|----------|------|
| `Developer` | `id`, `login`, `email` | `COMMITTED_BY` |
| `Repository` | `id`, `name`, `visibility`, `default_branch` | `CONTAINS` |
| `Commit` | `hash`, `timestamp`, `message` | `COMMITTED_BY`, `MODIFIED` |
| `File` | `path`, `language`, `loc` | `CONTAINS` â†’ `Class`, `Function` |
| `Class` | `name`, `file_path` | `INHERITS_FROM`, `IMPLEMENTS` |
| `Function` | `name`, `signature`, `file_path` | `CALLS` |
| `Module` | `name`, `type` | `IMPORTS` |

**ê·¸ë˜í”„ êµ¬ì¶• íŒŒì´í”„ë¼ì¸**:
```
1. íŒŒì‹± (Tree-sitter)
   â†“
2. AST ì¶”ì¶œ â†’ JSONL ìƒì„±
   (/mnt/efs/{analysis_id}/graph_nodes.jsonl, graph_edges.jsonl)
   â†“
3. ì ì¬ (Neo4j)
   - ì´ˆê¸°: neo4j-admin database import
   - ì¦ë¶„: graph_loader.py (Cypher UNWIND)
   â†“
4. ë²„ì „ ê´€ë¦¬
   - graph_snapshot_id â†’ PostgreSQL ì €ì¥
```

**ì‹œë§¨í‹± ì¸ë±ìŠ¤ íŒŒì´í”„ë¼ì¸**:
```
1. ì²­í‚¹ (ê¸°ë³¸ 200í† í°, ì˜¤ë²„ë© 50)
   â†“
2. ì„ë² ë”© ìƒì„±
   - AWS Bedrock (Titan Text Embeddings)
   - ë˜ëŠ” OpenAI (text-embedding-3-large)
   â†“
3. ë²¡í„° ì €ì¥
   - Amazon OpenSearch Serverless
   - ë©”íƒ€ë°ì´í„°: ê·¸ë˜í”„ ë…¸ë“œ ID ì°¸ì¡°
   â†“
4. ìºì‹± (S3)
   - ì»¤ë°‹ í•´ì‹œ ê¸°ë°˜ ì¬ì‚¬ìš©
```

**Graph-RAG ì¿¼ë¦¬ íë¦„**:
```
ì‚¬ìš©ì ì¿¼ë¦¬ (ìì—°ì–´)
   â†“
1. ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (Neo4j Cypher)
   - ì†Œìœ ê¶Œ, ê²°í•©ë„, í˜¸ì¶œ ê´€ê³„
   â†“
2. ì‹œë§¨í‹± ê²€ìƒ‰ (OpenSearch)
   - ìœ ì‚¬ ì½”ë“œ ë¸”ë¡ ê²€ìƒ‰
   â†“
3. ì»¨í…ìŠ¤íŠ¸ ê²°í•© â†’ LLM (Claude/GPT-4o)
   - ìµœì¢… ì¸ì‚¬ì´íŠ¸ ìƒì„±
```

### 3. ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œ (v2)

**L0 (í”„ë¡ íŠ¸ì—”ë“œ)**:
- React ëŒ€ì‹œë³´ë“œ â†’ `/api/v1/analysis/start` í˜¸ì¶œ
- ì €ì¥ì†Œ ë©”íƒ€ë°ì´í„° + í•„í„° ì „ì†¡

**L1 (ë°±ì—”ë“œ FastAPI)**:
- GitHub OAuth ìŠ¤ì½”í”„ ê²€ì¦
- ìºì‹œ ì¡°íšŒ (graph_snapshot í…Œì´ë¸”)
- ì‘ì—… í˜ì´ë¡œë“œ â†’ SQS ì „ì†¡
- `analysis_id` ê¸°ë¡

**L2 (Step Functions Map)**:
- `files_to_process`ë¥¼ ìµœëŒ€ ë™ì‹œì„± 100ìœ¼ë¡œ íŒ¬ì•„ì›ƒ
- ê° Batch Jobì— ì¶”ì  ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
- ì‹¤íŒ¨ í—ˆìš©ìœ¨ 5%

**L3 (Worker/Batch Job)**:
```python
# worker/run_analysis.py
def main():
    # 1. SQS ë©”ì‹œì§€ íŒŒì‹±
    job_data = json.loads(os.environ['SQS_MESSAGE_BODY'])

    # 2. EFS/S3ì—ì„œ ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ê°€ì ¸ì˜¤ê¸°
    repo_path = fetch_from_efs(job_data['analysis_id'])

    # 3. ê·¸ë˜í”„ ìŠ¬ë¼ì´ìŠ¤ ìƒì„±/ì¬í™œìš©
    graph_loader.load_or_reuse(job_data['commit_hash'])

    # 4. ì‹œë§¨í‹± ì¿¼ë¦¬ ì‹¤í–‰
    semantic_results = semantic_search.query(job_data['query'])

    # 5. PostgreSQL + Neo4jë¡œ ê²°ê³¼ ì „ì†¡
    save_results(job_data['analysis_id'], semantic_results)

    # 6. í—¬ìŠ¤ í”„ë¡œë¸Œ â†’ CloudWatch
    report_metrics()

    # 7. ì„±ê³µ ì‹œ SQS ë©”ì‹œì§€ ì‚­ì œ, ì‹¤íŒ¨ ì‹œ DLQ
```

**ì¬ì‹œë„ ë¡œì§**:
- ìµœëŒ€ 2íšŒ ìë™ ì¬ì‹œë„
- ì‹¤íŒ¨ ì‹œ DLQì— ê²©ë¦¬
- CloudWatch Alarm íŠ¸ë¦¬ê±°

### 4. Feature-Based ëª¨ë“ˆ êµ¬ì¡°

**ìë™ ë“±ë¡ íŒ¨í„´**: ê° FeatureëŠ” ë…ë¦½ì ìœ¼ë¡œ ë¼ìš°í„°ë¥¼ ë“±ë¡

**ë””ë ‰í† ë¦¬ êµ¬ì¡°**:
```
src/backend/features/v1/
â”œâ”€â”€ auth/                # ì¸ì¦ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ api.py           # APIRouter ì •ì˜
â”‚   â”œâ”€â”€ models.py        # SQLAlchemy ORM
â”‚   â”œâ”€â”€ schemas.py       # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â”œâ”€â”€ github_service.py
â”‚   â””â”€â”€ jwt_service.py
â”œâ”€â”€ github_analysis/     # ë¶„ì„ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ analysis_service.py
â”‚       â””â”€â”€ github_api_service.py
â””â”€â”€ webhooks/            # Webhook ëª¨ë“ˆ
    â”œâ”€â”€ api.py
    â”œâ”€â”€ models.py
    â””â”€â”€ schemas.py
```

**ìƒˆë¡œìš´ Feature ì¶”ê°€ ë°©ë²•**:
1. `features/v1/{feature_name}/` ë””ë ‰í† ë¦¬ ìƒì„±
2. `api.py`ì— `router = APIRouter(prefix="/api/v1/{name}", tags=["{name}"])` ì‘ì„±
3. `features/v1/__init__.py`ì—ì„œ importí•˜ë©´ ìë™ ë“±ë¡ë¨

### 5. ì˜ì¡´ì„± ì£¼ì… (Dependency Injection)

**FastAPI Depends()ë¡œ DB ì„¸ì…˜, ì¸ì¦, ì„œë¹„ìŠ¤ ì£¼ì…**

**DB ì„¸ì…˜ ê´€ë¦¬**:
```python
from common.dependencies import get_db
from sqlalchemy.orm import Session

@router.get("/users")
def list_users(db: Session = Depends(get_db)):
    users = db.query(User).all()
    return users
# ì„¸ì…˜ ìë™ commit/rollback/close
```

**í˜„ì¬ ì‚¬ìš©ì ì¸ì¦**:
```python
from common.dependencies import get_current_user

@router.get("/me")
def get_me(current_user: User = Depends(get_current_user)):
    return current_user
# JWT í† í° ê²€ì¦ í›„ User ê°ì²´ ë°˜í™˜
```

**TaskService ì£¼ì…**:
```python
from common.dependencies import get_task_service

@router.post("/analysis")
async def create_analysis(
    task_service: ITaskService = Depends(get_task_service)
):
    # í™˜ê²½ì— ë”°ë¼ LocalTaskService ë˜ëŠ” AwsBatchTaskService ì£¼ì…ë¨
    pass
```

### 6. Worker ë¶„ì„ ë¡œì§ (v2 ì—…ë°ì´íŠ¸)

**í˜„ì¬ ìƒíƒœ**: ê¸°ë³¸ git blame ë¡œì§ ì™¸ì— **Graph-RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„ í•„ìš”**

**í•„ìš” êµ¬í˜„**:

#### 6.1 `src/worker/analysis/git_analyzer.py` (ê¸°ë³¸)
```python
class GitAnalyzer:
    def clone_repository(self) -> str:
        """GitHub í† í°ìœ¼ë¡œ Private ì €ì¥ì†Œ í´ë¡ """
        # https://oauth2:TOKEN@github.com/user/repo.git í˜•ì‹ ì‚¬ìš©
        pass

    def analyze_blame(self, user_email: str) -> Dict[str, float]:
        """git blameìœ¼ë¡œ ì‚¬ìš©ì ê¸°ì—¬ë„ ê³„ì‚°"""
        # ëª¨ë“  íŒŒì¼ ìˆœíšŒí•˜ë©° git blame --line-porcelain ì‹¤í–‰
        # user_lines / total_lines ê³„ì‚°
        pass

    def analyze_tech_stack(self) -> Dict[str, any]:
        """íŒŒì¼ í™•ì¥ì ë° í”„ë ˆì„ì›Œí¬ ê°ì§€"""
        # .py, .js, .ts ë“± í™•ì¥ì ì¹´ìš´íŠ¸
        # package.json, requirements.txt ë“± í”„ë ˆì„ì›Œí¬ íŒŒì¼ ê°ì§€
        pass

    def cleanup(self):
        """ì„ì‹œ í´ë¡  ë””ë ‰í† ë¦¬ ì‚­ì œ"""
        pass
```

#### 6.2 `src/worker/analysis/graph_loader.py` (v2 ì‹ ê·œ)
```python
class GraphLoader:
    """Neo4j ê·¸ë˜í”„ ì ì¬ ë° ë²„ì „ ê´€ë¦¬"""

    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str):
        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))

    def parse_with_tree_sitter(self, file_path: str, language: str) -> Dict:
        """Tree-sitterë¡œ AST íŒŒì‹±"""
        # tree-sitter-python, tree-sitter-javascript ë“± ì‚¬ìš©
        # ë…¸ë“œ(Class, Function) + ì—£ì§€(CALLS, IMPORTS) ì¶”ì¶œ
        pass

    def stage_to_jsonl(self, nodes: List, edges: List, analysis_id: str):
        """EFSì— JSONL ìŠ¤í…Œì´ì§•"""
        # /mnt/efs/{analysis_id}/graph_nodes.jsonl
        # /mnt/efs/{analysis_id}/graph_edges.jsonl
        pass

    def bulk_load_to_neo4j(self, analysis_id: str):
        """Neo4j ëŒ€ëŸ‰ ì ì¬ (Cypher UNWIND)"""
        # UNWIND $nodes AS node CREATE (n:File {path: node.path, ...})
        # UNWIND $edges AS edge MATCH (a), (b) CREATE (a)-[:CALLS]->(b)
        pass

    def create_snapshot(self, commit_hash: str, analysis_id: str) -> str:
        """PostgreSQLì— graph_snapshot ë ˆì½”ë“œ ìƒì„±"""
        # graph_snapshot_id ë°˜í™˜
        pass

    def reuse_snapshot(self, commit_hash: str) -> Optional[str]:
        """ê¸°ì¡´ ìŠ¤ëƒ…ìƒ· ì¬ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        # ì»¤ë°‹ í•´ì‹œ ê¸°ë°˜ ìºì‹œ ì¡°íšŒ
        pass
```

#### 6.3 `src/worker/analysis/semantic_search.py` (v2 ì‹ ê·œ)
```python
class SemanticSearch:
    """ì‹œë§¨í‹± ì½”ë“œ ê²€ìƒ‰ (OpenSearch/Qdrant)"""

    def __init__(self, opensearch_endpoint: str, bedrock_client):
        self.opensearch = OpenSearch([opensearch_endpoint])
        self.bedrock = bedrock_client

    def chunk_code(self, file_content: str, chunk_size: int = 200, overlap: int = 50) -> List[str]:
        """ì½”ë“œ ì²­í‚¹"""
        # í•¨ìˆ˜ ë‹¨ìœ„ ë˜ëŠ” í† í° ë‹¨ìœ„ ì²­í‚¹
        pass

    def generate_embeddings(self, chunks: List[str]) -> List[np.ndarray]:
        """AWS Bedrock ì„ë² ë”© ìƒì„±"""
        # bedrock.invoke_model("amazon.titan-embed-text-v1")
        pass

    def index_to_opensearch(self, embeddings: List, metadata: List[Dict]):
        """OpenSearchì— ë²¡í„° ì¸ë±ì‹±"""
        # ë©”íƒ€ë°ì´í„°: graph_node_id, file_path, chunk_text
        pass

    def query(self, natural_language_query: str, k: int = 5) -> List[Dict]:
        """ìì—°ì–´ ì¿¼ë¦¬ë¡œ ìœ ì‚¬ ì½”ë“œ ê²€ìƒ‰"""
        query_embedding = self.generate_embeddings([natural_language_query])[0]
        results = self.opensearch.search(
            index="code_embeddings",
            body={"query": {"knn": {"vector": query_embedding, "k": k}}}
        )
        return results
```

**ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œ**: `docs/design/v2/PROJECT_PLAN_V2.md` ì°¸ì¡°

### 7. ì¸ì¦ ë° ë³´ì•ˆ

**GitHub OAuth 2.0 + JWT ì¸ì¦ í”Œë¡œìš°**:
```
1. ì‚¬ìš©ì "GitHub ë¡œê·¸ì¸" í´ë¦­
   â†“
2. Frontend â†’ Backend GET /api/v1/auth/github
   â†“
3. Backend â†’ GitHub OAuth í˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜
   â†“
4. GitHubì—ì„œ ì‚¬ìš©ì ê¶Œí•œ ìŠ¹ì¸
   â†“
5. GitHub â†’ Backend Callback (code í¬í•¨)
   â†“
6. Backend: code â†’ access_token êµí™˜
   â†“
7. Backend: GitHub APIë¡œ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ
   â†“
8. Backend: access_token ì•”í˜¸í™” ì €ì¥ (Fernet)
   â†“
9. Backend: JWT í† í° ìƒì„± (15ë¶„ ìœ íš¨)
   â†“
10. Frontend: JWT ì €ì¥ ë° API ìš”ì²­ ì‹œ Header í¬í•¨
```

**ì•”í˜¸í™”**:
- GitHub Access Token: Fernet ì•”í˜¸í™” í›„ DB ì €ì¥
- ì•”í˜¸í™” í‚¤: `.env`ì˜ `ENCRYPTION_KEY` (ìƒì„±: `Fernet.generate_key()`)

**JWT ì„¤ì •**:
- Access Token: 15ë¶„ ë§Œë£Œ
- Refresh Token: 7ì¼ ë§Œë£Œ
- Algorithm: HS256

---

## í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

### í•„ìˆ˜ `.env` ë³€ìˆ˜

**ë°ì´í„°ë² ì´ìŠ¤**:
```bash
POSTGRES_USER=sesami_user
POSTGRES_PASSWORD=sesami_password_2025
POSTGRES_DB=sesami_db
DATABASE_URL=postgresql://sesami_user:sesami_password_2025@db:5432/sesami_db
```

**Queue (Redis)**:
```bash
REDIS_HOST=queue
REDIS_PORT=6379
QUEUE_BROKER_URL=redis://queue:6379/0
CELERY_BROKER_URL=redis://queue:6379/0
CELERY_RESULT_BACKEND=redis://queue:6379/0
```

**Graph Store (Neo4j)** - v2 ì¶”ê°€:
```bash
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password
```

**Vector Store (OpenSearch)** - v2 ì¶”ê°€:
```bash
OPENSEARCH_ENDPOINT=https://opensearch:9200
OPENSEARCH_USER=admin
OPENSEARCH_PASSWORD=your_opensearch_password
```

**AWS Bedrock (ì„ë² ë”©)** - v2 ì¶”ê°€:
```bash
AWS_REGION=us-east-1
BEDROCK_MODEL_ID=amazon.titan-embed-text-v1
```

**GitHub OAuth**:
```bash
GITHUB_CLIENT_ID=your_github_client_id_here
GITHUB_CLIENT_SECRET=your_github_client_secret_here
GITHUB_REDIRECT_URI=http://localhost:3000/auth/callback
```

**ë³´ì•ˆ**:
```bash
JWT_SECRET_KEY=your-super-secret-jwt-key-change-this-in-production
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=15
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# Fernet ì•”í˜¸í™” í‚¤ ìƒì„±
ENCRYPTION_KEY=your-fernet-encryption-key-base64
```

**ì• í”Œë¦¬ì¼€ì´ì…˜**:
```bash
FRONTEND_URL=http://localhost:3000
BACKEND_URL=http://localhost:8000
TASK_SERVICE_IMPL=LOCAL  # ë˜ëŠ” AWS_BATCH
```

**Shared Storage (AWS)** - v2 ì¶”ê°€:
```bash
EFS_MOUNT_PATH=/mnt/efs
S3_EMBEDDING_CACHE_BUCKET=sesami-embeddings-cache
```

### ì•”í˜¸í™” í‚¤ ìƒì„±
```bash
python3 << 'EOF'
from cryptography.fernet import Fernet
key = Fernet.generate_key()
print(f"ENCRYPTION_KEY={key.decode()}")
EOF
```

---

## AWS ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

**ìƒì„¸ ë¬¸ì„œ**: `docs/design/v2/PROJECT_PLAN_V2.md`

### ì»´í¬ë„ŒíŠ¸ ë§¤í•‘ (v2 ì—…ë°ì´íŠ¸)

| ë¡œì»¬ í™˜ê²½ | AWS ì„œë¹„ìŠ¤ | ë³€ê²½ ì‚¬í•­ |
|----------|----------|----------|
| Frontend (React) | CloudFront + S3 | `npm run build` â†’ S3 ì—…ë¡œë“œ |
| Backend (FastAPI) | ECS on Fargate | Docker ì´ë¯¸ì§€ â†’ ECR â†’ ECS |
| Worker (Celery) | AWS Batch + Step Functions | `run_analysis.py` ì‘ì„±, L1/L2/L3 ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ |
| PostgreSQL | Amazon RDS | `pg_dump` â†’ RDS ë³µì› |
| Redis | Amazon SQS | TaskService êµ¬í˜„ì²´ êµì²´ |
| Neo4j (ë¡œì»¬) | Neo4j AuraDB | ë˜ëŠ” Neptune Serverless v2 |
| OpenSearch (ë¡œì»¬) | OpenSearch Serverless | ë²¡í„° ì¸ë±ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ |
| .env | Secrets Manager | í™˜ê²½ë³€ìˆ˜ â†’ Secrets ì´ê´€ |
| - | EFS | í´ë¡  ì €ì¥ì†Œ, JSONL ìŠ¤í…Œì´ì§• |
| - | S3 | ì„ë² ë”© ìºì‹œ, ì•„í‹°íŒ©íŠ¸ |

### AWS Step Functions ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (v2)

**L1 (ê¸€ë¡œë²Œ ì»¨íŠ¸ë¡¤ëŸ¬)**:
```json
{
  "Comment": "Sesami Analysis Orchestration",
  "StartAt": "ValidateInput",
  "States": {
    "ValidateInput": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:...:ValidateAnalysisRequest",
      "Next": "CheckCache"
    },
    "CheckCache": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:...:CheckGraphSnapshot",
      "Next": "FanOutFiles"
    },
    "FanOutFiles": {
      "Type": "Map",
      "ItemsPath": "$.files_to_process",
      "MaxConcurrency": 100,
      "Iterator": {
        "StartAt": "SubmitBatchJob",
        "States": {
          "SubmitBatchJob": {
            "Type": "Task",
            "Resource": "arn:aws:states:::batch:submitJob.sync",
            "Parameters": {
              "JobDefinition": "sesami-worker",
              "JobQueue": "sesami-job-queue",
              "ContainerOverrides": {
                "Environment": [
                  {"Name": "FILE_PATH", "Value.$": "$.file_path"},
                  {"Name": "ANALYSIS_ID", "Value.$": "$.analysis_id"}
                ]
              }
            },
            "Retry": [
              {
                "ErrorEquals": ["States.TaskFailed"],
                "MaxAttempts": 2,
                "BackoffRate": 2
              }
            ],
            "Catch": [
              {
                "ErrorEquals": ["States.ALL"],
                "ResultPath": "$.error",
                "Next": "LogToDLQ"
              }
            ],
            "End": true
          },
          "LogToDLQ": {
            "Type": "Task",
            "Resource": "arn:aws:lambda:...:SendToDLQ",
            "End": true
          }
        }
      },
      "Next": "AggregateResults"
    },
    "AggregateResults": {
      "Type": "Task",
      "Resource": "arn:aws:lambda:...:AggregateGraphResults",
      "End": true
    }
  }
}
```

### í•µì‹¬ ë³€ê²½ ì‚¬í•­

**1. AwsBatchTaskService êµ¬í˜„** (Phase 1):
```python
# src/backend/common/task_service/aws_batch_service.py
import boto3
import json

class AwsBatchTaskService(ITaskService):
    def __init__(self):
        self.sqs = boto3.client('sqs')
        self.sfn = boto3.client('stepfunctions')
        self.queue_url = os.environ['SQS_QUEUE_URL']
        self.state_machine_arn = os.environ['STEP_FUNCTIONS_ARN']

    async def enqueue_analysis(self, analysis_id, repo_url, target_user):
        # Step Functions ì‹¤í–‰ ì‹œì‘
        response = self.sfn.start_execution(
            stateMachineArn=self.state_machine_arn,
            input=json.dumps({
                'analysis_id': str(analysis_id),
                'repo_url': repo_url,
                'target_user': target_user
            })
        )
        return response['executionArn']
```

**2. AWS Batch Worker Entry Point** (Phase 3):
```python
# src/worker/run_analysis.py
import os
import json
import boto3
from analysis.git_analyzer import GitAnalyzer
from analysis.graph_loader import GraphLoader
from analysis.semantic_search import SemanticSearch

def main():
    # SQS ë©”ì‹œì§€ì—ì„œ ì‘ì—… ì •ë³´ íŒŒì‹±
    job_data = json.loads(os.environ['SQS_MESSAGE_BODY'])

    # Secrets Managerì—ì„œ GitHub í† í° ê°€ì ¸ì˜¤ê¸°
    secrets = boto3.client('secretsmanager')
    secret = secrets.get_secret_value(SecretId='github-token')
    access_token = json.loads(secret['SecretString'])['token']

    # EFSì—ì„œ ì €ì¥ì†Œ ìŠ¤ëƒ…ìƒ· ê°€ì ¸ì˜¤ê¸°
    efs_path = f"/mnt/efs/{job_data['analysis_id']}"

    # GraphLoaderë¡œ ê·¸ë˜í”„ êµ¬ì¶•
    graph_loader = GraphLoader(
        neo4j_uri=os.environ['NEO4J_URI'],
        neo4j_user=os.environ['NEO4J_USER'],
        neo4j_password=os.environ['NEO4J_PASSWORD']
    )

    # ì»¤ë°‹ í•´ì‹œ ê¸°ë°˜ ìºì‹œ í™•ì¸
    snapshot_id = graph_loader.reuse_snapshot(job_data['commit_hash'])
    if not snapshot_id:
        # ìƒˆë¡œìš´ ê·¸ë˜í”„ ë¹Œë“œ
        nodes, edges = graph_loader.parse_with_tree_sitter(efs_path)
        graph_loader.stage_to_jsonl(nodes, edges, job_data['analysis_id'])
        graph_loader.bulk_load_to_neo4j(job_data['analysis_id'])
        snapshot_id = graph_loader.create_snapshot(job_data['commit_hash'], job_data['analysis_id'])

    # SemanticSearchë¡œ ì„ë² ë”© ìƒì„± ë° ì¸ë±ì‹±
    semantic = SemanticSearch(
        opensearch_endpoint=os.environ['OPENSEARCH_ENDPOINT'],
        bedrock_client=boto3.client('bedrock-runtime')
    )
    chunks = semantic.chunk_code(file_content)
    embeddings = semantic.generate_embeddings(chunks)
    semantic.index_to_opensearch(embeddings, metadata)

    # ê²°ê³¼ ì €ì¥
    save_results(job_data['analysis_id'], snapshot_id)

    # CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡
    cloudwatch = boto3.client('cloudwatch')
    cloudwatch.put_metric_data(
        Namespace='Sesami/Worker',
        MetricData=[
            {'MetricName': 'GraphBuildTime', 'Value': elapsed_time, 'Unit': 'Seconds'},
            {'MetricName': 'EmbeddingCount', 'Value': len(embeddings), 'Unit': 'Count'}
        ]
    )

    # SQS ë©”ì‹œì§€ ì‚­ì œ
    sqs = boto3.client('sqs')
    sqs.delete_message(
        QueueUrl=os.environ['SQS_QUEUE_URL'],
        ReceiptHandle=os.environ['SQS_RECEIPT_HANDLE']
    )

if __name__ == '__main__':
    main()
```

**3. IaC (AWS CDK)** - Phase 3:
```typescript
// infra/cdk/stacks/sesami-stack.ts
import * as cdk from 'aws-cdk-lib';
import * as batch from 'aws-cdk-lib/aws-batch';
import * as ecs from 'aws-cdk-lib/aws-ecs';
import * as sfn from 'aws-cdk-lib/aws-stepfunctions';
import * as efs from 'aws-cdk-lib/aws-efs';

export class SesamiStack extends cdk.Stack {
  constructor(scope: cdk.App, id: string, props?: cdk.StackProps) {
    super(scope, id, props);

    // EFS for shared storage
    const fileSystem = new efs.FileSystem(this, 'SesamiEFS', {
      vpc: vpc,
      encrypted: true
    });

    // Batch Compute Environment
    const computeEnv = new batch.ComputeEnvironment(this, 'SesamiComputeEnv', {
      computeResources: {
        type: batch.ComputeResourceType.SPOT,
        maxvCpus: 256,
        instanceTypes: [new ec2.InstanceType('c5.xlarge')],
        vpc: vpc
      }
    });

    // Batch Job Definition
    const jobDef = new batch.JobDefinition(this, 'SesamiWorkerJob', {
      container: {
        image: ecs.ContainerImage.fromRegistry('sesami/worker:latest'),
        vcpus: 4,
        memoryLimitMiB: 8192,
        mountPoints: [{
          containerPath: '/mnt/efs',
          sourceVolume: 'efs'
        }],
        environment: {
          NEO4J_URI: neo4jSecret.secretValueFromJson('uri').toString(),
          OPENSEARCH_ENDPOINT: opensearchDomain.domainEndpoint
        }
      }
    });

    // Step Functions State Machine
    const orchestrator = new sfn.StateMachine(this, 'SesamiOrchestrator', {
      definition: l1ControllerChain,
      timeout: cdk.Duration.minutes(30)
    });
  }
}
```

---

## ì½”ë“œ êµ¬ì¡° ë° ê·œì¹™

### í”„ë¡œì íŠ¸ êµ¬ì¡° (v2 ì—…ë°ì´íŠ¸)
```
Sesami/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ design/
â”‚       â”œâ”€â”€ v1/                  # ê¸°ë¡ ë³´ì¡´ìš©
â”‚       â””â”€â”€ v2/                  # ğŸ¯ ìµœì‹  ê³„íšì„œ (Graph-RAG)
â”‚           â”œâ”€â”€ PROJECT_PLAN_V2.md
â”‚           â””â”€â”€ README.md
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ cdk/                     # AWS CDK IaC (Phase 3 ìƒì„±)
â”‚       â”œâ”€â”€ bin/
â”‚       â”œâ”€â”€ lib/
â”‚       â””â”€â”€ stacks/
â”‚           â””â”€â”€ sesami-stack.ts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ frontend/                # React + Vite
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ contexts/
â”‚   â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Analysis/   # ê·¸ë˜í”„ ì¸ì‚¬ì´íŠ¸ UI (v2)
â”‚   â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”œâ”€â”€ backend/                 # FastAPI
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dependencies.py
â”‚   â”‚   â”‚   â”œâ”€â”€ encryption.py
â”‚   â”‚   â”‚   â”œâ”€â”€ exceptions.py
â”‚   â”‚   â”‚   â””â”€â”€ task_service/
â”‚   â”‚   â”‚       â”œâ”€â”€ base.py
â”‚   â”‚   â”‚       â”œâ”€â”€ local_service.py
â”‚   â”‚   â”‚       â””â”€â”€ aws_batch_service.py  # ğŸ”¨ Phase 1
â”‚   â”‚   â””â”€â”€ features/v1/
â”‚   â”‚       â”œâ”€â”€ auth/
â”‚   â”‚       â”œâ”€â”€ github_analysis/
â”‚   â”‚       â”‚   â””â”€â”€ api.py        # /insights ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ (v2)
â”‚   â”‚       â””â”€â”€ webhooks/
â”‚   â””â”€â”€ worker/                  # Celery Worker
â”‚       â”œâ”€â”€ celery_app.py
â”‚       â”œâ”€â”€ tasks.py
â”‚       â”œâ”€â”€ database.py
â”‚       â”œâ”€â”€ run_analysis.py      # AWS Batch Entry Point (Phase 3)
â”‚       â””â”€â”€ analysis/
â”‚           â”œâ”€â”€ git_analyzer.py
â”‚           â”œâ”€â”€ graph_loader.py  # ğŸ”¨ Phase 2 (Tree-sitter + Neo4j)
â”‚           â””â”€â”€ semantic_search.py  # ğŸ”¨ Phase 2 (Bedrock + OpenSearch)
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env
```

### Naming Conventions
- **Python**: `snake_case` (í•¨ìˆ˜/ë³€ìˆ˜), `PascalCase` (í´ë˜ìŠ¤)
- **TypeScript**: `camelCase` (í•¨ìˆ˜/ë³€ìˆ˜), `PascalCase` (ì»´í¬ë„ŒíŠ¸/íƒ€ì…)
- **íŒŒì¼ëª…**:
  - Python: `snake_case.py`
  - React ì»´í¬ë„ŒíŠ¸: `PascalCase.tsx`
  - ìœ í‹¸ë¦¬í‹°: `camelCase.ts`

### Import ìˆœì„œ
```python
# 1. í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import json
from typing import Dict, List

# 2. ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from neo4j import GraphDatabase  # v2 ì¶”ê°€

# 3. ë¡œì»¬ ê³µí†µ ëª¨ë“ˆ
from common.database import get_db
from common.exceptions import NotFoundException

# 4. ë¡œì»¬ Feature ëª¨ë“ˆ
from .models import User
from .schemas import UserResponse
```

### ì—ëŸ¬ ì²˜ë¦¬
```python
# src/backend/common/exceptions.py
from fastapi import HTTPException, status

class NotFoundException(HTTPException):
    def __init__(self, detail: str):
        super().__init__(status_code=404, detail=detail)

class UnauthorizedException(HTTPException):
    def __init__(self, detail: str):
        super().__init__(status_code=401, detail=detail)

class GraphLoadException(HTTPException):  # v2 ì¶”ê°€
    def __init__(self, detail: str):
        super().__init__(status_code=500, detail=f"Graph load failed: {detail}")

# ì‚¬ìš© ì˜ˆì‹œ
raise NotFoundException(detail="User not found")
raise GraphLoadException(detail="Neo4j connection timeout")
```

---

## ì„¤ê³„ ë¬¸ì„œ ì°¸ì¡°

**â­ v2 ìš°ì„  ì°¸ì¡°**: `docs/design/v2/`

| ë¬¸ì„œ | ë‚´ìš© | ë²„ì „ |
|-----|------|------|
| `v2/PROJECT_PLAN_V2.md` | **Graph-RAG ì¢…í•© ê³„íšì„œ** (â­ ìµœìš°ì„  ì°¸ì¡°) | v2 |
| `v2/README.md` | v1/v2 ì°¨ì´ì  ìš”ì•½ | v2 |
| `v1/00_OVERVIEW.md` | í”„ë¡œì íŠ¸ ëª©í‘œ, ê¸°ìˆ  ìŠ¤íƒ (ê¸°ë¡ìš©) | v1 |
| `v1/01_SYSTEM_ARCHITECTURE.md` | ê¸°ë³¸ 3-tier ì•„í‚¤í…ì²˜ | v1 |
| `v1/02_LOCAL_DEVELOPMENT.md` | ë¡œì»¬ í™˜ê²½ êµ¬ì¶• | v1 |
| `v1/03_AWS_MIGRATION.md` | AWS ê¸°ë³¸ ì¸í”„ë¼ (v2ì—ì„œ í™•ì¥) | v1 |
| `v1/06_IMPLEMENTATION_PLAN.md` | 12ì£¼ ê¸°ë³¸ ë¡œë“œë§µ (v2ì—ì„œ ì¬ì¡°ì •) | v1 |

**ì°¸ì¡° ìš°ì„ ìˆœìœ„**:
1. **Graph-RAG/ì‹œë§¨í‹± ê²€ìƒ‰**: `v2/PROJECT_PLAN_V2.md` (ì„¹ì…˜ 4)
2. **ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œ**: `v2/PROJECT_PLAN_V2.md` (ì„¹ì…˜ 3)
3. **12ì£¼ ë¡œë“œë§µ**: `v2/PROJECT_PLAN_V2.md` (ì„¹ì…˜ 6)
4. **ìš´ì˜ ê°€ë“œë ˆì¼**: `v2/PROJECT_PLAN_V2.md` (ì„¹ì…˜ 7)
5. **ê¸°ë³¸ ì•„í‚¤í…ì²˜**: `v1/01_SYSTEM_ARCHITECTURE.md`

---

## í˜„ì¬ êµ¬í˜„ ìƒíƒœ (v2 ì—…ë°ì´íŠ¸)

### âœ… êµ¬í˜„ ì™„ë£Œ
- Docker Compose ë©€í‹° ì„œë¹„ìŠ¤ í™˜ê²½
- FastAPI Backend êµ¬ì¡°
- React Frontend ê¸°ë³¸ êµ¬ì¡°
- GitHub OAuth ì¸ì¦ í”Œë¡œìš°
- JWT í† í° ì‹œìŠ¤í…œ
- TaskService ì¶”ìƒí™” ì¸í„°í˜ì´ìŠ¤
- LocalTaskService (Celery)
- ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ ê¸°ë³¸ êµ¬ì¡°

### ğŸ”¨ êµ¬í˜„ í•„ìš” (v2 ë¡œë“œë§µ)

**Phase 1 (1~2ì£¼) - ê¸°ë°˜**:
- [ ] `graph_snapshot` í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜
- [ ] `AwsBatchTaskService` êµ¬í˜„
- [ ] ë¡œì»¬ Neo4j + OpenSearch ì»¨í…Œì´ë„ˆ êµ¬ì„±
- [ ] `make graph-dev` í—¬í¼ ì¶”ê°€

**Phase 2 (3~5ì£¼) - Graph-RAG**:
- [ ] Tree-sitter íŒŒì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (`graph_loader.py`)
- [ ] Neo4j ëŒ€ëŸ‰ ì ì¬ + ë¡¤ë°± ë¡œì§
- [ ] Bedrock ì„ë² ë”© íŒŒì´í”„ë¼ì¸ (`semantic_search.py`)
- [ ] ê·¸ë˜í”„ ì¸ì‚¬ì´íŠ¸ UI ì»´í¬ë„ŒíŠ¸

**Phase 3 (6~8ì£¼) - AWS ì—°ë™**:
- [ ] AWS CDK - Step Functions (L1/L2)
- [ ] Batch Job Definition + ECR ì´ë¯¸ì§€
- [ ] CloudWatch ë©”íŠ¸ë¦­/ì•ŒëŒ
- [ ] `/api/v1/analysis/{id}/insights` API

**Phase 4 (9~10ì£¼) - ì•ˆì •ì„±**:
- [ ] ê·¸ë˜í”„ ìŠ¤ëƒ…ìƒ· ì¬ì‚¬ìš© (ì»¤ë°‹ í•´ì‹œ ìºì‹±)
- [ ] ì¹´ì˜¤ìŠ¤ í…ŒìŠ¤íŠ¸ + ì¬ì‹œë„ ê²€ì¦
- [ ] X-Ray + OpenTelemetry ë¶„ì‚° ì¶”ì 

**Phase 5 (11~12ì£¼) - ì¶œì‹œ**:
- [ ] ë³´ì•ˆ ì·¨ì•½ì  ì ê²€
- [ ] GitHub Actions CI/CD (OIDC)
- [ ] íŒŒì¼ëŸ¿ ë¶„ì„ + ë©”íŠ¸ë¦­ ì •ë¦¬

### ğŸ’¡ í–¥í›„ ê°œì„  ì‚¬í•­
- LLM ê¸°ë°˜ ì •ì„± í‰ê°€ (Claude/GPT-4o í†µí•©)
- ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸ (WebSocket)
- íŒ€ í˜‘ì—… ê¸°ëŠ¥
- ë¦¬í¬íŠ¸ PDF ë‚´ë³´ë‚´ê¸°

---

## ìš´ì˜ ê°€ë“œë ˆì¼ (v2)

### ì‹œí¬ë¦¿ ê´€ë¦¬
- âš ï¸ **ì ˆëŒ€ ê¸ˆì§€**: GitHub/DB ìê²© ì¦ëª…ì„ ì´ë¯¸ì§€ì— í¬í•¨
- **ë¡œì»¬**: `.env` íŒŒì¼ (`.gitignore` í•„ìˆ˜)
- **AWS**: Secrets Manager ë˜ëŠ” SSM Parameter Store

### ìŠ¤í‚¤ë§ˆ ë³€ê²½ í”„ë¡œì„¸ìŠ¤
1. `docs/design/v2/PROJECT_PLAN_V2.md` ì—…ë°ì´íŠ¸
2. Alembic ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„± ë° í…ŒìŠ¤íŠ¸
3. `MIGRATION_GUIDE.md` ë˜ëŠ” `UUID_MIGRATION.md` ì—…ë°ì´íŠ¸
4. PR ìƒì„± ë° ë¦¬ë·°

### í…ŒìŠ¤íŠ¸ ìµœì†Œ ê¸°ì¤€
- **Backend**: `pytest` (ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ)
- **Frontend**: `npm run lint && npm run test`
- **í†µí•©**: `make ci` ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸

### ê´€ì¸¡ì„± í‘œì¤€
- **ë¡œê·¸**: JSON êµ¬ì¡° ë¡œê·¸ + Correlation ID
- **ë©”íŠ¸ë¦­**: CloudWatch (AWS) / Prometheus (ë¡œì»¬)
- **ì¶”ì **: X-Ray + OpenTelemetry
- **ì•ŒëŒ**: í ê¹Šì´, ê·¸ë˜í”„ ë¹Œë“œ ì‹œê°„, ì‹¤íŒ¨ìœ¨

---

## Troubleshooting

### í¬íŠ¸ ì¶©ëŒ
```bash
lsof -i :3000  # Frontend
lsof -i :8000  # Backend
lsof -i :5432  # PostgreSQL
lsof -i :7474  # Neo4j Browser
lsof -i :9200  # OpenSearch
kill -9 <PID>
```

### ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
```bash
# âš ï¸ ëª¨ë“  ë°ì´í„° ì‚­ì œ (PostgreSQL + Neo4j + OpenSearch)
docker-compose down -v
docker-compose up --build
```

### Workerê°€ ì‘ì—…ì„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
```bash
# Celery ìƒíƒœ í™•ì¸
docker-compose exec worker celery -A celery_app inspect active

# Redis ì—°ê²° í™•ì¸
docker-compose exec queue redis-cli ping

# íì— ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
docker-compose exec queue redis-cli LLEN celery
```

### Neo4j ì—°ê²° ì‹¤íŒ¨ (v2)
```bash
# Neo4j ìƒíƒœ í™•ì¸
docker-compose exec neo4j cypher-shell -u neo4j -p password "MATCH (n) RETURN count(n);"

# ë¡œê·¸ í™•ì¸
docker-compose logs neo4j

# ë°ì´í„°ë² ì´ìŠ¤ ì¬ì‹œì‘
docker-compose restart neo4j
```

### OpenSearch ì¸ë±ì‹± ì‹¤íŒ¨ (v2)
```bash
# OpenSearch ìƒíƒœ í™•ì¸
curl -X GET "http://localhost:9200/_cluster/health?pretty"

# ì¸ë±ìŠ¤ ëª©ë¡
curl -X GET "http://localhost:9200/_cat/indices?v"

# íŠ¹ì • ì¸ë±ìŠ¤ í™•ì¸
curl -X GET "http://localhost:9200/code_embeddings/_search?pretty"
```

### ì»¨í…Œì´ë„ˆ ë¹Œë“œ ì‹¤íŒ¨
```bash
# ìºì‹œ ì—†ì´ ì¬ë¹Œë“œ
docker-compose build --no-cache backend
docker-compose build --no-cache worker
docker-compose up --build --force-recreate
```

### ë§ˆì´ê·¸ë ˆì´ì…˜ ë¬¸ì œ
```bash
# í˜„ì¬ ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒíƒœ
docker-compose exec backend alembic current

# ë§ˆì´ê·¸ë ˆì´ì…˜ íˆìŠ¤í† ë¦¬
docker-compose exec backend alembic history

# íŠ¹ì • ë¦¬ë¹„ì „ìœ¼ë¡œ ì´ë™
docker-compose exec backend alembic downgrade <revision>

# graph_snapshot í…Œì´ë¸” í™•ì¸
docker-compose exec backend python -c "
from common.database import SessionLocal
db = SessionLocal()
result = db.execute('SELECT * FROM graph_snapshot LIMIT 5').fetchall()
print(result)
"
```

### AWS Batch ë””ë²„ê¹… (v2)
```bash
# CloudWatch Logs í™•ì¸
aws logs tail /aws/batch/sesami-worker --follow

# Batch Job ìƒíƒœ
aws batch describe-jobs --jobs <job-id>

# Step Functions ì‹¤í–‰ ê¸°ë¡
aws stepfunctions describe-execution --execution-arn <arn>
```
